\chapter{Conclusion}\label{ch:conclusion}
Pushing the limits of computational optical microscopy requires a unique blend of both hardware and software innovations; among a wide variety of techniques for probing a sample, coded illumination using a programmable light source is advantageous in it's simple implementation and wide utility for various computational imaging techniques. In this work, we proposed a variety of techniques which take advantage of programmable illumination to enable label-free contrast, quantitative imaging, and high-throughput imaging at relatively low cost and complexity on existing optical systems. We have leveraged novel hardware and computational techniques to push performance boundaries in critical areas, such as label-free imaging and neuropathology. Further, we have put significant effort towards quantifying when computational techniques do not necessarily provide benefit, and acknowledge limiting cases where conventional methods are still state-of-the-art. Taken together, the methods presented in this work illustrate the capabilities of a computational imaging system, and offer evidence of the impact these systems could have on the broader optical imaging community.

In Chapter~\ref{ch:phase}, we introduced label-free phase imaging, specifically differential-phase contrast (DPC). After describing the DPC technique and it's limitations, we introduced a single-shot method for quantitative phase and amplitude imaging which uses partially-coherent (angle-coded) multiplexed color illumination to recover the linearized optical field from a single deconvolution was presented. Our hardware requirements are simple, inexpensive, and compatible with most commercial microscopes through of the use of a color camera and a simple color filter insert placed at the back focal plane of the condenser lens, the same position as many removable phase contrast annulus rings. Unlike phase contrast and DIC, our method does not require special objectives or prisms, which reduces our hardware costs to that of the 3D printed filter itself. In addition, we can use our quantitative phase and amplitude methods to synthesize phase contrast and DIC images digitally, matching the functionality of existing phase imaging systems at a fraction of the cost.

In Chapter~\ref{ch:highthroughput}, we introduced the concept of high-throughput imaging, and described conventional methods for obtaining images with a wide field-of-view and high resolution. After discussing the limitations of existing methods, we proposed a novel coded illumination technique where multiple motion-blurred images are acquired while a sample is in motion. These images, having higher SNR than images captured at the same speed under strobed illumination, are then computationally deblurred to recover the static object with a much wider FOV than before, while maintaining the resolution of the imaging optics and having a relevant SNR for applications such as cell-counting or neuropathology. We compare our technique to existing methods, stop-and-stare and strobed illumination, which require a significantly longer acquisition time or produce images with significantly lower SNR, respectively. Using a generalized framework for predicting the reconstruction SNR of measurements acquired using each method, we show, through both theory and experiment, that our coded illumination technique can produce images with up to 10$\times$ the SNR of strobed acquisitions, at significantly faster acquisition rates than images captured under strobed illumination. The magnitude of this benefit is not constant across system parameters such as camera readout noise and illumination power; to quantify how our method compares to existing methods, we perform a longitudinal method comparison, providing context for a system-builder which will help guide their decision to implement our technique on practical systems. For low-light situations such as fluorescence imaging, our method is uniquely suited to provide faster imaging at higher SNR than existing methods.

In Chapter~\ref{ch:fabrication}, we describe the design process for devices enabling computational illumination, such as a programmable LED dome and high-speed illumination device as well as a portable microscopy platform which uses a programmable LED dome. We begin by laying out common design requirements of domed LED devices, before detailing several design iterations of these LED domes which were developed along the way. In the first design iteration, we designed and fabricated a 3D-printed LED dome which was carefully assembled by hand, requiring several months of fabrication. This device was intended to be used with CellScope, a portable microscope platform which uses a smartphone to capture and process images for telemedicine applications. In the second design iteration, we designed and developed a LED quasi-dome which uses 5 printed-circuit boards arranged in a dome-like structure. This device requires significantly less time and effort to assemble, and provides RGB illumination across 581 LEDs up to 0.9NA. The ease of manufacturing has enabled the wide distribution of these devices to collaborators around the world. Finally, we describe the Computational CellScope platform in greater detail, which implements coded illumination on a portable platform to perform digital refocusing using light-field methods, quantitative phase imaging, and multi-contrast imaging.

In Chapter ~\ref{ch:selfcal}, two methods for self-calibration of computational imaging systems are described. Computational imaging methods are uniquely acceptable to mis-calibration since the inferred forward model is often assumed to be ideally calibrated. In reality, the forward model is a function of the optical physics and optical design (which can be inferred), as well as mis-calibration, which cannot be assumed to be negligible in many system. We first discuss a technique for performing aberration self-calibration using differential phase contrast. Our method employs an alternating-minimization approach to solve for both the complex field of the object as well as the aberrations, which are projected into a Zernike basis. This method is non-convex, but is differentiable, and guaranteed to to diverge so long as an appropriate step size is used. Further, we use a patch-wise solver to solve for aberrations across the field, revealing spatially variant aberration functions. We verify our method by solving for the defocus aberration when introducing a known defocus term. Our method requires only a single coherent measurement, in addition to three DPC measurements for recovering the object's complex field. Next, we presented a method for performing self-calibration of LED positions in both the brightfield and darkfield regions for Fourier ptychography. After performing a brightfield calibration method, we employ a novel gradient-based technique for solving for the homography between printed circuit boards in a quasi-dome device. This technique makes the alternating minimization process much more stable, and enables high-NA Fourier ptychography with resolution below 450nm. While a good hardware calibration is most effective, these self-calibration techniques can mitigate mis-calibration when this is not possible.

\section{Proposed Future Work}
As my time at Berkeley comes to a close, there are still many open questions which I find interesting. Some of these might be considered continuations of this work, and some may yet be completely new directions that have revealed themselves along this highly nonlinear journey. In the future, I hope that myself or someone else will have the time and a motivation to explore a few of the following proposals.

- Motion deblurring using compressed sensing, and unrolled networks
- Active acquisitions - determining where to image in real-time for fast, task-based imaging such as cell counting
- Self-calibration: Motion deblur
- Fabrication: Cellscope?

The first, and most general, is the application of a task-based imaging paradigm to the applications discussed in this work. In each of the previous chapters, we have quantified the quality of our reconstructions using system parameters such as signal-to-noise ratio, acquisition throughput (space-bandwidth rate), resolving power and field of view. Rarely is emphasis placed on the quality of these images towards a specific task, such as cell counting, pathogen recognition, or a diagnosis of disease or condition. Conventionally the field of computational imaging has tended to focus on the former batch of metrics, but with machine learning expertise becoming more common, I think finding a performance metric (cost function) which is both relevant to the imaging task and differentiable would be extremely interesting. As a first step, I would propose cell-counting as a beachhead metric. Recent work from the machine learning community has proposed optimizing network layers for optimized counting and classification of cells~\cite{falk2019unet, xue2017cell}. Including programmable hardware elements (such as LED arrays which could introduce different contrast) into the learning pipeline could provide significant improvements in counting accuracy and performance.

A second, more specific future direction, is the application of compressed sensing to solve under-determined motion deblurring problems for slide-scanning and digital pathology applications. Currently, the volume of data required for large-scale imaging (e.g. neuropathology) can make acquisition times and data storage difficult or infeasible. If data acquisition processing requirements were lower, acquisitions could be made much faster, and large-scale imaging analysis could more readily performed. For a compressed-sensing acquisition to work, measurements must be made in a method which introduces aliasing which is incoherent with a domain in which the sample is sparse. An acquisition with a random forward operator is ideal, because this matrix is incoherent with every other matrix. In our case, we cannot create a truly random operator, but we have control over the structure of our forward operator through the illumination sequence as well as the motion pathway - so long as this acquisition introduces aliasing which is incoherent, we have some hope of reconstructing a sparse object from an under-determined system. To accomplish this, we propose optimizing the entire pipeline using an unrolled network, which allows us to differentiate with simple parameters such as step size as well as more complicated parameters such as the sparsifying operator, or even the acquisition strategy. Using large amounts of training data we could then generate a principled acquisition and reconstruction strategy together which would enhance the capabilities of high-throughput acquisitions well-beyond those presented in this work.

A third future direction is active imaging, or the active determination of acquisition trajectories based on current image data. These trajectories need not span Cartesian space, but could also include LEDs, defocused positions, or other metrics. A key component to these methods being advantageous would be that the sample should be localized, meaning that most of the densely sampled data is unnecessary, and that the learning process should be fast, so that the compute + scan time does not exceed the time to densely scan a sample. An obvious example of this would be multi-well plates, where a microscope may perform a fast scan over many wells which aliases them together, then uses this blurred data to determine which wells are worth imaging more closely at high-resolution for quantification. Such a technique could speed up acquisitions significantly for many applications.

In terms of fabrication of LED devices, I believe that engineering improvements in the power supply of the devices as well as a more careful layout of the high-speed traces (such as serial clock and PWM clock) would enable imaging with shorter exposure times. In addition, a more compact portable microscope device incorporating a PCB-based LED array would be much more robust than the current Computational CellScope device, enabling wider distribution to collaborators around the world.
