\chapter{A Smartphone-Based Portable Computational Imaging System}
ttt
\section{Portable Microscopy}
Optical microscopy is an important tool for disease screening and diagnosis throughout the world. Significant resources have been devoted to developing portable and affordable compact microscopes for remote clinical applications~\cite{Zhu2011, switz2014low, smith2011cell, maamari2013mobile,C4LC00010B,Vashist2014,steenblik2005lenses,10.1371/journal.pone.0098781,boppart2014point,Greenbaum17122014,mudanyali2010compact,tseng2010lensfree}. Compact microscopes based on mobile phones, including CellScope~\cite{Breslauer:09,Sk:14, have demonstrated that microscopy can be effectively performed outside of hospitals and diagnostic laboratories by minimally trained healthcare workers, that images can be transmitted for confirmation of diagnosis, and that phone-based computational analysis can be used to provide automated diagnosis. These mobile microscopes complement a host of other new devices for health monitoring on smart phones [3].

\section{Computational CellScope}
Here, a new variation of the CellScope microscope is demonstrated which incorporates recently developed techniques of computational illumination~\cite{Zheng2011, Tian14, zijiMulti} to enable new imaging modalities, including darkfield, phase imaging and digital refocusing. Using the same LED array illumination, Computational CellScope also implements lightfield digital refocusing, so that a sample focus can be changed after the fact (without mechanically changing focus) and 3D image stacks can be extracted for both intensity and phase modes. Further, constant focus correction (auto-focusing) can be implemented in post-processing for long time-lapse studies. The digital refocusing is achieved by sequentially illuminating the sample from each of the LEDs that lie inside the numerical aperture (NA) of the objective, then post-processing to form a stack of through-focus images of intensity~\cite{Ng2005,Zheng2011} or phase contrast~\cite{Tian14}. For thick samples, the result also provides a 3D reconstruction of the sample, similar to limited angle tomography.

The computational illumination techniques used here have been previously demonstrated in a traditional microscope using a planar LED array ~\cite{Zheng2011,Zheng2013,Tian14,zijiMulti,tian20153d}. The purpose of the LED array is to flexibly pattern illumination angles at the sample by turning on different sets of LEDs corresponding to different illumination angles. The optimal arrangement of LEDs, however, is not planar but rather a dome shape ~\cite{Dominguez:14}, which we utilize here. The domed arrangement provides significant improvements in intensity uniformity and light throughput, since LEDs can be directionally biased and arranged at uniform radius from the sample. These benefits contribute to increased signal-to-noise ratio (SNR) in the darkfield images, allowing effective high angle illumination patterning and shorter exposure times.

The flexibility and speed of the programmable LED array illuminator, as well as the lack of moving parts and low cost, make the hardware very amenable to modification as a CellScope attachment. In order for our device to be practically useful in the field, we have here enforced the requirement that all of our processing and control be performed on the smartphone, without use of a PC. Thus, the device can be field-deployable as a simple add-on to CellScope. In the following sections we detail the design and performance of the hardware and software of our new Computational CellScope device.

\begin{figure} [h]
\begin{center}
\includegraphics[width=\textwidth]{ccs-fig1.png}
\end{center}
\caption {{Computational CellScope.} {a).} Device observing a sample using a Nexus 4 smartphone. {b).} Optical schematic of the CellScope device with our custom-made domed LED illuminator. {c).} CAD assembly of the dome. {d).} Assembled dome and control circuitry.}
\label{fig:device}
\end{figure}

\section{Domed LED Illumination}
The Computational CellScope hardware involves a custom-built domed LED illuminator attached to an inverted variant of the CellScope smartphone-based microscope platform (see Fig. 1). The CellScope used here is a finite-conjugate transmission microscope coupled to an Android-based Nexus 5 smartphone (LG Electronics/Google) as described in in Skandarajah, et al.~\cite{Sk:14. Our domed illuminator hardware is compatible with all smartphones and tablets that are used with the existing CellScope, including the iPhone 4S, 5, 5S, and 6 (Apple, Inc.), as well as several Android devices. Phones are mounted via modular 3D printed mounts adapted to each specific smartphone model. Hardware changes were entirely on the illumination side, where we have replaced the original single LED light with our domed illuminator consisting of 508 individually addressable broad spectrum (white) LEDs. Our domed LED arrangement was inspired by the opto-mechanical geometry of the AWARE gigapixel camera~\cite{brady2012multiscale}. LEDs are uniformly distributed in an (approximately) hexagonal packing pattern across a 77 degree cone of angles corresponding to an illumination NA of 0.62. Thus, darkfield imaging is feasible for objectives with NA smaller than 0.62 (as illustrated in Fig.~\ref{fig:dome}f), and both phase and digital refocusing are possible for all objective NAs. The dome assembly was is secured to a custom stage that attached to the top of the CellScope objective; the stage and circuit board holders were 3D printed using low-cost ABS plastic. In general, the design is modular and features simple electronics, including the use of the inexpensive and widely used Arduino micro-controller platform. Phone mounts can be swapped out for upgrading to new models and objectives can be replaced for varying the magnification of the system. While our addition involves custom LED drive circuitry and a 3D printed structure, complexity was kept low to preserve the low-cost nature of CellScope. Part counts, cost and especially size may be further reduced in design-for-manufacture. The size of the illuminator could be reduced to essentially the dimensions of the dome itself, and cost could be comparable to the price of a modern smartphone, matching and improving upon the functionality of a full-size microscope at a fraction of the cost.

% Dome design figure
\begin{figure} [h]
\begin{center}
\includegraphics[width=\textwidth]{ccs-fig2.png}
\end{center}
\caption {{ Domed LED Illuminator.}{a)} Illumination pattern used to acquire dark field images with a 0.25 NA objective.
{b)} Illumination pattern used to synthesize differential phase contrast images with a 0.25 NA objective.
{c)} Illustration of the arbitrary illumination patterning capabilities of the device.
{d)} Normalized mean pixel intensities measured at the sensor for the planar and domed arrays. Intensity decreases as a function of angle in both cases, but much more strongly in the case of the planar geometry. Values were normalized to the central LED's brightness in both cases.
{e)} Visual comparison of a planar LED array with a domed array. Since the intensity of a spherical wave drops as a function of the inverse square of radius, the illumination at the sample depends on the distance between the LEDs and the sample. In the planar case (left), LED distance $r$ increases as a function of illumination angle, causing weaker illumination at higher angles. A domed LED array (right) eliminates this variation ($r$ is constant).
{f)} Plot illustrating the relative objective NA for several common magnifications, as compared to our dome's LED placement (small black circles).
{g)} Normalized measured intensity falloff as a function of angle relative to the optical axis for the domed and planar LED arrays. Falloff is proportional to $\cos{\theta}$ for the domed geometry and $\sim\cos^4{\theta}$ for the planar geometry. Black lines are $\cos{\theta}$ and $\cos^4{\theta}$ fits for the domed and planar geometries, respectively. The domed geometry exhibits significant improvements in intensity at large angles of illumination.
}
\label{fig:dome}
\end{figure}

The domed LED arrangement provides significantly better light efficiency than the planar LED arrays used in previous work, enabling shorter acquisition times and more efficient power use. These advantages could be crucial for mobile microscopy applications where power is a scarce resource, and shorter exposure times reduce motion blur artifacts due to unstable experimental conditions. The power benefits are a result of two phenomena, shown in Fig. ~\ref{fig:dome}e. The first is that off-axis LEDs in a planar array will have a larger LED-to-sample distance and thus decreased intensity at the sample. For example, if we assume that each LED is a point emitter, the intensity falloff due to increased distance can be expressed as $I(\theta) = I_0 \cos^2{\theta}$, where $I_0$ is the intensity at the sample from the on-axis LED and $θ$ is illumination angle. The second improvement in light efficiency comes from the fact that LEDs have significant angular variation in intensity (typically emitting more light in the forward direction). In a planar array, the LEDs at higher angles provide less effective illumination, a problem corrected by the dome geometry, where all LEDs are radially oriented. In both the domed and planar geometries we note that intensity further decreases with a final factor of $\cos{\theta}$ due to the smaller profile of objective window when viewed off-axis; combining these factors and assuming a Lambertian ($\sim\cos{\theta}$) angular dependence for physical (non-point-source) LEDs results in an expected intensity falloff of $\sim\cos^4{\theta}$ for the planar geometry but only $\sim\cos{\theta}$ for the domed geometry, a vast improvement at high incidence angles. Thus, the difference between geometries is proportional to $cos^3{\theta}$, or a factor of $> 50\%$ at $40\degree$ and $99\%$ at $77\degree$ incidence, having a substantial impact on required exposure times. Such behavior matches well with our experimental measurements (Figs.~\ref{fig:dome}d,g), where the measured intensity is shown for both geometries out to 40˚ incidence. Variations in intensity between LEDs may also come from electrical variations such as batch differences in controller chips and resistor tolerances.

% Contrast method comparison figure
\begin{figure}
\begin{center}
\includegraphics[width=0.67\textwidth]{ccs-fig4.png}
\end{center}
\caption {{Image Results Compared to a Standard Microscope.} Computational CellScope acquires brightfield and darkfield images of similar quality to a standard upright microscope (Nikon TE300) without the use of hardware inserts. Additionally, it enables phase imaging using Differential Phase Contrast (DPC), which contains similar information to standard phase contrast imaging, and can be inverted to obtain quantitative phase of the sample (bottom row). Differences in color shades are caused by the relative differences in hue of the halogen lamp and the white LEDs. Note the additional dark features in DIC results, as compared to DPC, illustrating mixing of phase and absorption information in DIC. In the rightmost column, we show images for an unstained transparent sample, illustrating the utility of phase imaging methods for label-free imaging.
}
\label{fig:contrastcomparison}
\end{figure}


% \section{Multi-Contrast Imaging}

% To achieve brightfield, darkfield and phase contrast simultaneously, we time-multiplex images taken with different LED patterns and post-process them on the smartphone to synthesize pseudo-real-time multi-contrast imaging, as in~\cite{zijiMulti}. Brightfield images correspond to illumination by LEDs that lie within the cone of angles described by the objective numerical aperture (NA). Darkfield images are obtained by illuminating the sample from angles beyond the angular acceptance of the objective (Fig.~\ref{fig:dome}a)~\cite{Zheng2011}. Since different objectives have different NA, one must specify in the software which objective is being used, with larger NA corresponding to a larger brightfield region of LEDs. Our dome is designed to enable darkfield contrast for any objective of NA$<0.62$, roughly corresponding to a typical 40$\times$ objective.

% Phase contrast can be achieved in a single-shot image by any asymmetric illumination pattern~\cite{kachar1985asymmetric,Dodt01101999}. Here, we choose to employ a differential phase contrast (DPC) scheme~\cite{Hamilton1984a,mehta2009quantitative,Tian14,ford2012phase}, which requires two images having complementary illumination patterns, because it gives good phase contrast at all spatial frequencies and can be quantitatively interpreted. The method involves sequentially illuminating the sample with the two opposite halves of the brightfield circle while capturing an intensity image for each. For example, one may first take an image, $I_\mathrm{R}$, with only the right half of the LEDs on and then a second image, $I_\mathrm{L}$, with only the left half of LEDs on (see Fig.~\ref{fig:dome}b). The two images are processed as follows to obtain brightfield and phase contrast:

% \begin{equation}
% I_{\mathrm{BF}}=I_\mathrm{L}+I_\mathrm{R}, \qquad I_{\mathrm{DPC}}= \frac{I_\mathrm{L}-I_\mathrm{R}}{I_\mathrm{L}+I_\mathrm{R}},
% \label{IBF}
% \end{equation}

% \noindent where $I_{\mathrm{BF}}$ is the brightfield image and $I_{\mathrm{DPC}}$ is the phase contrast image. Since the LEDs are mutually incoherent, adding the two images gives an equivalent brightfield image and subtracting them produces phase contrast, due to asymmetric clipping in Fourier space. The intensity of the DPC image can be shown to be approximately proportional to the first derivative of phase along the direction of illumination asymmetry~\cite{Hamilton1984a}, and different axes of rotation can be programmed by changing the LED array pattern accordingly. Typically, we capture an additional two images in order to compute both the Left-Right and Top-Bottom phase derivative results representing both orthogonal directions. DPC images are qualitatively similar to Differential Interference Contrast (DIC); however, the latter is not a quantitative method. To obtain quantitative phase from DPC images, we solve the inverse problem~\cite{mehta2009quantitative,tian20153d} using a simple deconvolution in Fourier space, as shown in Fig.~\ref{fig:contrastcomparison}.

% Thus, by acquiring two (or four) half-brightfield images and a single darkfield image for each time point, we can synthesize brightfield, darkfield, and phase contrast modes in near real-time. Users have the option of saving and post-processing time-multiplexed frames or viewing a live multi-contrast display of the sample, though display speed is significantly faster in the latter case. We developed an application to stream these four contrast modes size-by-side while updating each frame sequentially as the illumination pattern cycles through the different patterns (Fig.~\ref{fig:android}b). The user may touch any of the four images for a live full-screen display of that contrast mode only, and the illumination pattern cycle will update to reflect this.

% Some image results for each of the contrast modes are shown in Fig.~\ref{fig:contrastcomparison}, using different objective magnifications and samples. For comparison, we show the same samples imaged in a commercial inverted microscope with traditional hardware. Darkfield was obtained by using a Ph3 condenser aperture in combination with objectives having NA smaller than the sine of the half-angle of the Ph3 annulus inner diameter. Since DPC is not currently commercially available, we instead compare our DPC phase contrast images to (similar-appearing) DIC. Both provide images whose contrast is related to the first derivative of phase along a single direction; however, DIC mixes absorption and birefringence information with phase, so that dark features in the image may result from either absorption of the sample or phase contrast interferences. In the DPC images, on the other hand, the image is related purely to the sample phase distribution (see Fig.~\ref{fig:contrastcomparison}), which can be inverted to reveal quantitative phase, as shown in the bottom row. Provided in a portable package, these multi-contrast video and streaming methods have the potential to allow clinicians to view a sample with three separate contrast methods at once, enhancing the information available for diagnosis and disease discrimination.

% \section{Digital Refocusing}
% For thick samples, our system can capture a different sequence of images in order to recover 3D images and enable digital refocusing. In this case, we sequentially capture images for each of the LEDs in the brightfield region. The resulting dataset is similar to limited angle tomography with many angles, which provides depth sectioning from angular information~\cite{Kak:1988fk}. For simpler processing more amenable to mobile phone programming, we use a lightfield approach here ~\cite{Ng2005,Zheng2011}. This involves a simple shift-and-add algorithm to digitally refocus the image to different axial ($z$) planes. We calculate the digitally refocused intensity image at a distance $\Delta z$ away from the physical focus plane as:
% \begin{equation}
% I^{\Delta z} = \sum_{\text{all brightfield LEDs}}I_i(x+\Delta z\tan{\theta_x}, y+\Delta z\tan{\theta_y}),
% \label{I_refocus}
% \end{equation}
% where $I_i$ denotes the intensity image for the $i^{\text{th}}$ LED, shifted according to its angle of illumination at the sample $(\theta_x,\theta_y)$ and the desired refocus distance $\Delta z$.

% The number of individual LEDs making up the brightfield region roughly determines the number of depth planes that can be accurately reconstructed, and the range of illumination angles determines the axial resolution of the 3D result. Conveniently, the illumination angles may be flexibly sub-sampled in order to trade off acquisition time for quality of result. Since a separate image is taken for each illumination angle, both acquisition and processing time are a function of the numerical aperture of the objective, as illustrated in Fig.~\ref{fig:android}. Acquisition speed was primarily limited by the time required to save an image to the smartphone’s flash memory at full resolution (8 Megapixels on the Nexus 4). This is important because data acquisition remains fast, while processing can occur in the background. Using the same dataset, we can also calculate 3D phase contrast images by digitally refocusing the two halves of the brightfield region separately~\cite{Tian14}. It is expected that this mode of imaging intensity or phase in 3D with no moving parts will give better diagnostic information for thick samples. Alternatively, it could be used for correcting misfocus, obviating the need for automatic axial translation or automated focus adjustment in long time-lapse studies.

% % Digital refocus results figure
% \begin{figure}
% \begin{center}
% \includegraphics[width=\textwidth]{ccs-fig5.png}
% \end{center}

% \caption { {Digital refocusing on the Computational CellScope.} Comparison of digital refocusing to physical refocusing on a commercial microscope (Nikon TE300) of a house fly wing sample (AmScope PS200) with a 10$\times$ objective. Digitally refocused phase contrast images are also computed for both vertical and horizontal phase derivatives at different focus depths.}

% \label{fig:digrefocus}
% \end{figure}

% Results are shown in Fig.~\ref{fig:digrefocus} for digitally refocused images as compared to physically refocused images on an inverted microscope (Nikon TE300), both with a 10$\times$ objective (0.25 NA). The phase contrast images show the first derivative of phase along both the vertical and horizontal directions, calculated from the same dataset using only the green color channel. The algorithm successfully refocused features across 400$\mu$m depth of field, limited by object thickness. Our refocusing achieves an axial resolution of approximately 5$\mu$m within $\pm50\mu$m of physical focus position, but degrades approximately linearly with increasing refocus distance~\cite{Tian14}. Processing time is approximately 1.5 minutes per depth slice for a 10$\times$ objective.

% \section{Hardware Implementation}
% The illuminator consists of 4 major components: a hemispherical dome frame for mounting the LEDs, the LEDs themselves, controller circuit boards and the sample stage. The dome mounting structure is a rigid hemisphere designed to constrain the individual LEDs within an array of computationally positioned bores, aligning the LED with the radius vector to the sample center.  This hemisphere was designed with a $60\textrm{mm}$ radius in order to provide maximum intensity at the sample, given our desired number of LEDs and a minimum distance between neighboring LEDs. The part was 3D printed using a SLA printer (InterPro Models) to achieve the necessary 100$\mu$m printing resolution for accurate LED positioning. The LED angular positions were computed algorithmically to ensure uniform spacing across the dome, constrained by a minimum 150$\mu$m distance between bores for mechanical rigidity and a maximum angular separation of 3.85 degrees allowing for sufficient coherence area at the sample. This angular spacing means that 38 LEDs make up the brightfield region for our smallest NA objective (4$\times$), with even more for larger NA objectives, ensuring high quality digital refocusing results across a large range of depth slices for all objectives. The $3\textrm{mm}$ through-hole, white LEDs (Mouser 593-VAOL-3LWY4) were press fit into the dome and a rigid lateral constraint was provided by acrylic retaining inserts behind each individual LED. 508 of these LEDs were soldered directly to controller boards arranged above the array, with insulated leads to prevent electrical shorting.

% Accounting for mechanical tolerances of the 3D printed dome and the LED epoxy lenses, manufacturing tolerances suggest that the maximum angular pointing error will be no greater than ±4.8 degrees. This corresponds to a maximum intensity attenuation of only 1.2\% due to assembly variation and tolerances across all illumination angles. Our illumination is also quite uniform across the field of view. The maximum field of view of our optical system has a radius of $1.25\textrm{mm}$, set by the eyepiece field-stop diameter of $10\textrm{mm}$ and assuming a 4$\times$ objective. Given the $60\textrm{mm}$ radius of curvature of our dome, this corresponds to illumination variation due to mechanical tolerances being less than 1\% across the field of view for each single LED illumination.  While this result is quite good, the spread of intensities between different LEDs is significantly larger (see Fig.~\ref{fig:dome}g), as a result of combined mechanical, electrical, and parts tolerances. Conveniently, a one-time calibration sweep of illumination angles, taken with no sample present, is sufficient to allow computational removal of this variation for all practical purposes.

% The device used nine identical printed circuit boards placed in a fanned arrangement above the dome, each containing four LED controller chips (Texas Instruments TLC5926) serving up to 64 LEDs. These were controlled by a single Arduino Micro micro-controller, which calculates the appropriate bit pattern based on serial commands from an included Bluetooth transceiver. The array is fully addressable through a standard Bluetooth serial link; no wired connection to the phone is needed, although a powered USB connection is provided to charge the phone’s battery as well, for convenience. We operate the serial output at 115K baud and note that we can update the entire pattern with approximately 100ms latency, although we predefine some of the more complex LED illumination patterns and store them in the Arduino flash memory to further improve acquisition time. Thus our final acquisition time is primarily limited by the smartphone camera rather than the LED array control, so could be improved significantly with future smartphone releases.

% The dome's power control board is tolerant of voltages between 7 and 20 VDC to allow compatibility with a large range of power sources, including a standard 12V automotive battery and a 100W wall-plug variable output power supply, as well as many commercially available portable power supplies for consumer electronics. During regular usage, the device requires no more than 2A of current, though it could potentially draw up to 4.8A of current when all LEDs are illuminated. This is not a typical use case, however, since simultaneous illumination inside and outside the objective NA amounts to an undesirable mixing of darkfield and brightfield contrast. Noting that for 4$\times$, 10$\times$, and 20$\times$ objective configurations there are more darkfield than brightfield LEDs, to reduce power consumption we perform darkfield illumination by default using an annulus with a width equivalent to 0.15NA rather than using all of the darkfield LEDs. This moderately reduces the contrast and resolution of darkfield images but significantly reduces power use during the darkfield illumination cycle. We note that the device can operate indefinitely without overheating issues for both multi-contrast and digital refocusing.

% % Application Timing and Screenshot figure
% \begin{figure}
% \begin{center}
% \includegraphics[width=\textwidth]{ccs-fig3.png}
% \end{center}
% \caption {{Android Application Workflow.} {a).} Schematic of streaming multi-contrast LED patterns. Here we vary the LED pattern in time and acquire and process images on the smartphone, producing a streaming multi-contrast display of a sample without any further post-processing. The user can touch any image to zoom in and stream an individual image. Total cycle time is 2.3 seconds.
% {b).} Overview of workflow for digital refocusing mode. Table shows example processing and acquisition times for a typical dataset reconstruction. Axial Resolution is determined by the range of illumination angles sampled (defined by the objective NA). The number of z-steps were chosen such that refocus blur does not exceed 20 pixels. Processing and acquisition time can be reduced by selecting fewer refocus planes or by sparsely sampling LEDs, trading axial resolution for faster acquisition time.}
% %, roughly ¼ the size of a typical 20 $\mu$m cell at 10$\times$

% \label{fig:android}
% \end{figure}

% \subsection{Acquisition Software and Processing}
% It has previously been shown that using a smartphone as a microscope poses unique challenges intrinsic to the phone software~\cite{Sk:14. Smartphone cameras may only allow minimal quantitative control over standard imaging parameters (e.g. focus, exposure, gain), opting for opaque automatic algorithms that simplify the user experience. To circumvent these restrictions, we wrote a custom Android application that attempts to achieve the optimum imaging characteristics with our coded illumination configuration. In addition to a global tap-to-auto-focus capability, specific settings for each acquisition mode are detailed where appropriate in the following discussion. The software application also initiates and handles the Bluetooth connection to the domed array, enabling synchronized acquisition and array control through the standard Android API. Array control is thus transparent to the end-user, requiring them to simply pair the phone with the illuminator and press a connect button to initiate a Bluetooth connection. Our application was developed specifically for the Android platform, and will be compatible with any phone running Android OS version 4.0 or later. However, our algorithms were developed using the OpenCV Library, which is cross-platform for iOS (Apple, Inc.) and other operating systems. Thus most of our application code is portable to other smartphone platforms with moderate development effort. A screenshot of the acquisition and processing are shown in Fig.~\ref{fig:android}. The user may choose to collect and synthesize any or all of darkfield, refocused brightfield and DPC images.

% In our application, images were acquired using the standard Android API, which does not provide an interface to set explicit exposure times in lieu of auto-exposure with predefined exposure offset values that are only effective while auto-exposure is active. To circumvent this issue, we included a short pre-illumination sequence before each dataset acquisition to lock exposure at the appropriate value. Additionally, to account for the asymmetric LED packing of our LED array, we choose equal numbers of LEDs for each half-circle used to form DPC images, since DPC requires symmetric and equal illumination. Finally, we incur significant latency between the camera shutter and the availability of the frame to our application within the API, due to post-processing algorithms integral to the phone and performed in the background (e.g. white balance and demosaicing). This severely limits our acquisition speeds, which will likely be improved in newer versions of Android that allow finer camera control through the API. Apple iOS offers a different camera API that may also offer improvements in acquisition speed.

% Data post-processing was performed in a standalone Android app, where image stacks were loaded and processed on the phone. We employ a number of functions of the OpenCV Imaging library for Android to perform most of our computation. Individual DPC images are computed in less than a second, as demonstrated in our multi-contrast view mode. Digitally refocusing an image into 21 depth planes (±100 $\mu$m range with 10 $\mu$m sectioning) requires approximately 30 minutes of processing time, but the resulting 3D image stack can be interacted with in real-time; all other computational imaging results are much faster (~ 0.43 frames/sec). The long processing time is attributed to frequent loading and saving to the smartphone’s internal storage. We note that significant improvement in processing speeds for all of our algorithms is possible through implementation of our algorithms using the Android NDK, and is also expected as phone computational power increases with each product generation. These performance metrics were calculated on a Nexus 5 smartphone (LG Electronics) and may vary on other devices.


% % \section{Design of a Domed LED Illuminator for High-Angle Computational Illumination}

% % Computation Illumination using an array of Light Emitting Diodes (LEDs ) is a new approach to improving and expanding the functionality of existing microscopes, replacing the standard condenser illumination used for centuries [1-3]. Contrast in optical microscopy is conventionally obtained in a variety of ways – for purely absorptive samples, bright field microscopy images the light that is attenuated by the sample by illuminating within the range of angles defined by the numerical aperture (NA) of the objective. Conversely, dark field microscopy uses illumination from angles outside of the illumination NA, imaging only light that is scattered or “bent” by a refractive medium such as water. Rather than relying on expensive illumination hardware to block transmitted light at the pupil plane, the simple addition of an LED array enables both of these modalities by dynamically changing the pattern via software [1,2].
% % Computational Illumination using a LED array enables qQuantitative phase recovery through with an LED array microscopy involves methods such as Fourier ptychography Ptychography [4,5] and differential phase contrast  [2,3], enabling the measurement of the dry mass of many aqueous biological samples [2]. Differential Phase Contrast (DPC) incorporates uses two images with asymmetric illumination to recover the derivative of phase across a single dimension, which may be inverted to recover absolute phase [3]. By time-multiplexing brightfield, darkfield, and DPC patterns we can also generate streaming multi-contrast videos of samples [6]. By time multiplexing darkfield, brightfield, and phase constant patterns, it is possible to generate multi-contrast time series, taking advantage of the programmable nature of the LED Array [4]. Fourier Ptychography is a well-demonstrated method of obtaining pixel super-resolution beyond the diffraction limit of the objective used by sequentially scanning illumination angles, corresponding to a pupil shift in Fourier space, which may then be used to synthesize super-resolution images using information from all LEDs and stitching the images together in Fourier space, similar to synthetic aperture [544]. The resulting images have the same magnification and field of view as the low-magnification objective used to collect the data, but also with a synthetically higher NA, equal to the sum of the illumination and objective NA. Thus, by increasing our illumination NA we can obtain maximumsignificantly improve resolution (~5-7x in practice) by capturing many images. This method also has the advantage of recovering the phase of the sample, as well as the amplitude and phase of the pupil function, describing all aberrations of the imaging system, during the same process.
% % Previous computational illumination techniques have utilized a planar array, with a few notable exceptions, which were not programmable [675]. Here we present the design of a domed variant of the conventional planar LED array illuminator to improve upon light efficiency in the LED array microscope. the capabilities of many of these techniques which was demonstrated in our previous work on a mobile, point of care microscope employing computational illumination [7]. A domed illuminator is the optimal configuration because we desire even homogenous LED intensity across all illumination angles  and the planar array suffers two losses of light for high angle illumination. First, the planar array means that LEDs illuminating the sample from high angles are further away from the sample, and so the intensity of the light reaching the sample , which falls off approximately with as the square of the radius between the LED and sample (assuming for approximately point source, surface mount LEDs). Further, the LEDs we use do not emit at all angles isotopically, providing more intensity at smaller angles  (Fig. 1A). Thus, the higher angles have further reduced intensity in the planar case. This severe loss of light at high angle illumination is particularly problematic for Uniform, strong illumination at high angles is important for Fourier Ptychography, which depends on  images illuminated from high angles to obtain maximum resolution. Such darkfield images already have low photon counts and so planar arrays necessitate very long exposure times, severely limiting acquisition times. The domed illuminator, on the other hand, improves illumination uniformity in two ways: the use ofuses through-hole, directional LEDs instead of point sources to concentrate more illumination in the direction of the sample, and by mechanically positioning the LEDspositioned at even equal distances from the sample, correcting both types of light loss, rather than further with increasing angle, as is the case with a planer array. Practically, however, illumination will still decrease with cos⁡(θ) in both cases due to the reduced area of the objective window projected onto the axis of illumination, as shown in Fig 1. . This effect is seen experimentally in Fig. 1B. A prototype dome was built and tested in in our previous work on a mobile, point of care microscope employing computational illumination [86]. This dome design is shown in Figures 1B, 2B, and 2C. Still – wFigure 1B  shows s e noteintensity measurements for a range of angles, demonstrating a significant improvement in the mean intensity of illumination between planar and domed arrays. Here, we extend the design to a larger range of angles and optimized angular sampling.

% %
% %
% % Fig. 1: AB. Schematic comparison of planar and domed LED arrays.
% % BA. Relative mean pixel intensities (as measured at the CCDcamera) of constructed planar vs domed arrays as a function of LED angles  ..
% % B. Schematic comparison of planar and domed LED arrays.
% %
% % WHere we detail our design, fabrication, calibration techniques for a domed LED array in the context of Fourier Ptychography, but with other techniques employing computational illumination in mind. The designs presented here should be generalizable to LED arrays intended for reflective microscopy as well as other applications outside of conventional microscopy.
% % 2.  Design
% % To develop a feasible dome design problem, it is first necessary to define our appropriate constraints for hole position. First, for Fourier Ptychography to function, we have previously determined that approximately a 60% pupil overlap in Fourier space between adjacent LED coverage is necessary for the iterative algorithm to converge. The size of the pupil is set by the NA of the objective; therefore, this overlap must be defined for the smallest NA objective of the system. From this requirement, we determine the maximum hole spacing across the array using the following formula:
% % d_max=2r(1-0.6)NA_objective
% % Where  NA_objective is smallest objective NA of that will be used in the system, r is the dome radius, and d_max is the maximum distance between adjacent holes for the given dome radius.  The minimum hole spacing is determined by the minimum distance the LEDs can be placed apart without mechanically intersection , including a small additional space for structural rigidity. Using these criteria, an appropriate minimum dome radius is chosen such that both constraints are satisfied . In practice this radius should be minimized as LED intensity falls off with increasing distance from the sample, but sufficient range between the maximum and minimum spacing criteria should be provided so that hole packing algorithms are feasible. For the domed LED Array shown in Fig. 1B, 2B and 2C, we used a radius if 60mm and constrained our LED spacing to values between 4.8mm and 3.3mm, determined by desired pupil overlap and mechanical separation respectively.
% % The problem of packing points evenly across a sphere is well-studied but currently does not have a closed-form solution. Several methods have been developed to approximate equal packing using deterministic methods   such as the Saff-Kuijlaars method [9]. However, we found that these solutions fail to find an optimal global packing algorithm without further optimization. Initial attempts to generate points spaced evenly across the dome involved using hexagonal close-packing, as was used in the AWARE Gigapixel Camera [108]. However, this packing arrangement becomes problematic at high angles, as it does not take into account angular deficiencies that can be reduced by incorporating pentagons into the design.
% % The fundamental problem we wish to solve is that of finding N equally spaced points on a sphere that satisfy the pairwise constraints.To produce even packing across the entire dome we developed We obtain our solution numerically, through an iterative procedure drawing upon two distinct models of this problem. From a standpoint of electrostatics, we can view this problem as finding a stationary distribution of N point charges on a spherical conducting shell . This gives us a very natural way of updating our system of particles, using small perturbations in the directions of net Coulomb forces and projecting points back onto the unit ball. The second model we employ is that of uniform triangulation of parametric surfaces . Considering the Delaunay triangulation of the surface points  gives us a relatively fast method of checking whether or not we have satisfied our constraints in a manner that is more robust to numerical non-convergence than the Coulomb conducting shell model  .
% % 	Further, we determined that quadrant symmetry is of significant importance for differential phase contrast, as well for array alignment purposes. To account for this constraint, we predefine and rigidly constrain all points along the x and y Cartesian axes prior to optimization. After an optimum positioning has been reached, we use our packing metric to determine the most optimum quadrant of the full sphere of points, and reflect this across the rigid boundaries to form a symmetric packing layout. At this point we truncate the sphere to a dome determined by the desired  iIllumination NA. In this way we may arbitrarily define our imaging NA depending on our application, independent of our packing algorithm.
% %
% %
% %
% % Fig. 2: A. Our LED array microscope with the current planar array installed. B. Example Point Cloud used for designing the LED placement in the domed LED illuminator. C. Corresponding CAD Dome Design D. Simulation of source recovery   from calibration routine across a ± 30 deg. pattern. E. LED Pattern displayed on prototype domed array.
% %
% % Fig. 2: A. Example Point Cloud used for designing the LED placement in the domed LED illuminator. B. Corresponding CAD Dome Design C. Actual LED Pattern displayed on prototype domed array. D. Simulation of source recovery  from calibration routine.
% % 3.  Assembly
% % Our dome is intended to beDomes are 3D-printed using an opaque plastic material (Interpro Models)   ; we have found that thisblack material helps  is necessary to minimize both reflection as well as as scattering causing LEDs illuminating to illuminate neighboring channels. These issues can be particularly problematic for dark field imaging, where it is necessary to illuminate exclusively outside the NA of the objective since bright field illumination would quickly saturate the image. The resolution of the printer should be around 100-200μm depending on the mounting features used to retain the LEDs. Retention methods should be developed depending on the LED type; for single-channel LEDs we found that inserting a plastic insert behind each LED will sufficiently constrain each LED, while providing simple installation. To retain each LED we used a plastic insert which is press-fit behind the LED. For multi-channel LEDs it is necessary to provide rigid constraint using the flange of the LED, or a similar plastic insert on either side of the leads  .The LEDs are controlled via serial network of constant-current LED drivers and Serial PC interface via Arduino Microcontroller.
% % 4.  Calibration
% % The Fourier Ptychography algorithm is particularly sensitive to position accuracy of LEDs at high angles or at far distances from focus for the 3D Fourier Ptychography case  [5] . To verify our LED placement we developed a source estimation algorithm which uses optimization techniques to accurately locate each LED relative to the rest of the array. The method captures a set of intensity images at different focus positions using  To calibrate the angular position of each LED, we use a mechanical Zz-stage (Thorlabs) to move our sample axially relative to the dome, placing a test target at the sample plane for structure in the image. At each position, we collect images with each single LED, noting that by geometric optics the sample will be shifted as determined by the illumination angle and the distance from the sample   by the equation:
% % Δx=Δz tan⁡〖θ_x 〗      Δy=Δz tan⁡〖θ_y 〗
% % Where θ_x and θ_y describe the angle of illumination relative to the optical axis and Δz is the distance from the focal plane. . Using these this data, we compute a least-squares fit of the relative shift between each axial position for a single LED, which we can use to compute the absolute angular position of the LED within the array. Repeating this for all LEDs, we build up a map of relative positions of each LED. Additionally, we can determine the radius of each LED using a similar algorithm. By formulating the calibration procedure as an optimization problem we provide support for many kinds of mechanical errors that could be encountered during assembly, allowing us to synthesize synthetic super-resolution images with greater accuracy. This calibration, along with sufficient global alignment, is essential for effective phase imaging using Fourier Ptychography.


% \section{Quasi-Dome}
% Computational imaging involves the concurrent and jointly optimized design of both imaging hardware and post-processing algorithms to enable fast, simple, and high-quality imaging modalities. In microscopy, computational imaging using coded illumination has found a wide range of applicability for multi-contrast imaging~\cite{Zheng2011, zijiMulti, Phillips15}, super-resolution imaging~\cite{Zheng2013, Tian14}, label-free quantitative phase imaging~\cite{tian2015quantitative, JingsanSourceRecovery2016, Phillips:17}, and 3D Imaging~\cite{Tian3dDpc}. Conventionally, LED array illumination was used to quickly probe the sample across multiple frames, using different static illumination patterns during each frame. Inspired by works from the field of computational photography~\cite{raskar2006coded, agrawal2009optimal}, recent work demonstrated that using dynamic illumination patterns \textit{within} the exposure time can be paired with global sample motion to enable the recovery of a static object from a single blurred measurement in an optical microscope~\cite{Ma:15}. In this a proof of concept, a motion stage was configured to move the sample at known velocity during a single-frame, while the global intensity of a the LED illumination was varied in symphony. This combination of sample motion, illumination coding, and camera exposure enabled the recovery of the static sample from a blurred image. It is important to note that in this implementation the sample motion was assumed to be known, unlike many blind motion-deblurring works from the field of computational photography~\cite{nayarDeblur, shan2008high, levin2006blind,caiWavelet}. This assumption is is well-supported since the motion pathway of the motion stage was pre-programmed, under the condition of hardware triggering between components and accurate system alignment.

% Importantly, the quality recovery of a static object from a blurred image is very dependent on the design of the illumination sequence during the exposure time, and must be optimized to provide acceptable reconstruction. In previous work~\cite{raskar2006coded, agrawal2009optimal, Ma:15} the global illumination sequence within each exposure was optimized for minimum condition number of the forward operator. In this work, we extend these methods to the case of an array of angularly distributed sources, increasing the dimensionality of the motion deblurring problem in order to further reduce the condition number of the forward operator. In minimizing the condition number, we improve the SNR of our computational reconstruction, enabling shorter exposure times and faster motion imaging strategies.
% \section{Spectrally Variant Motion Deblurring}
% \subsection{Relating SNR to Motion Deconvolution}
% Motion blurring can be modeled as a convolutional system $y=Ax + \eta$, where $A$ is a circulant convolution matrix and $\eta$ is assumed to be white gaussian noise. To invert $A$, a pseudo-inverse may be used to minimize the $\ell-2$ distance between the measurements, $y$, which can be used to find an estimate of the object, $x^*$:

% \begin{equation}\label{eq:inversion}
% x^* = A^{-1} A x + A^{-1}\eta \approx (A^H A)^{-1} A^H y + (A^H A)^{-1} A^H \eta
% \end{equation}

% Intuitively, the error in this estimate, $e$, is a function of both measurement noise $\eta$ and a transformation imposed by the forward operator:
% \begin{equation}
% e = x^* - x =  (A^H A)^{-1} A^H \eta
% \end{equation}
% Given the variance of $\eta$ and the singular values of $A$, an estimate of the signal-to-noise ratio (SNR) may be obtained~\cite{agrawal2009optimal}. Conveniently, the circulant forward operator $A$ may be diagonalized by the Fourier transform, revealing the eigenvalues, $\tilde{b}$, which are the vectorized Fourier transformation of the blur kernel $b$. This enables efficient calculation of the singular values of $C$ based the spectrum of the convolution kernel ($\sigma^2_A = |\tilde{b}|^2$) of the blur kernel $b$ using a Fourier transform. In previous work~\cite{agrawal2009optimal}, it was shown that the reconstruction SNR was proportional to the sum of inverse singular values of the blur kernel:
% \begin{equation}
% SNR\propto \sum_i^N \frac{1}{\sigma_i} = \sum_i^N \frac{1}{|\tilde{b}_i|}
% \end{equation}

% \subsection{Spectrum of Motion Blur and LED Illumination}
% A general motion pathway can de described as a sum of infinity small Kronecker delta functions; In the case of a band-limited system, a minimum feature size is imposed by the Nyquist criterion. For a fully sampled optical system this allows the blur kernel to be written as a discrete sum of Kronecker delta functions, or their Fourier transform:

% \begin{equation}\label{eq:blur}
% b = \sum_i^N b_i \cdot \delta(\vec{r} - \vec{r}_i) = \mathscr{F}^{-1}\{\sum_i^N b_i \cdot e^{-i2\pi \vec{r} \vec{r}_i}\}
% \end{equation}

% Where $N$ is the number of discretized positions in the blur kernel, $b_i$ and $\vec{r}_i$ are the illumination coefficients and cartesian positions of the $i^{th}$ element in the blur pathway. \\
% The Weak-Object Transfer Functions (WOTFs) are formed as an approximation of the
