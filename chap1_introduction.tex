\chapter{Introduction}\label{ch:introduction}

\section{Optical Microscopy}

The optical microscope is one of the oldest scientific instruments, and continues to be an essential tool for researchers, medical professionals, and engineers across many disciplines. Microscopes are typically defined as having two or more refractive surfaces to provide magnification between the object of interest and the imaging plane, enabling the user to see things much smaller than the resolving power of the human eye. Credit for the invention of the compound microscope is generally attributed to Hans and Zacharias Jansen~\cite{van2010origins}, although the first published work on microscope design wasn't released until 1665 (Hooke and van Leeuwenhoek) \cite{natureMilestones,hookeMicrographica}. The term "microscope" is generally used to describe optical microscopes - those which are designed for use with light within the optical band of the electro-magnetic spectrum ($390nm \leq \lambda \leq 700nm$), which is approximately the electromagnetic spectrum detectable by the human eye.

\subsection{Imaging and Resolution}
Light interacts with our world in many ways, including diffraction, refraction, reflection, and absorption. At optical wavelengths, many common materials (such as glass) have favorable properties for refractive optics (providing significant phase delay with little absorption), facilitating precise control of an optical wavefront using these elements. Imaging is the process of creating a copy of a particular optical signal at a different position in space, typically with the goal of making a measurement using a film or electronic detector. In the simplest case, a single lens may be used to form a magnified image of an optical signal by placing the lens and detector at a particular distance from the sample A single-lens imaging system has many practical issues, however, including lacking telecentricity (consistency of magnification across the field) and aberrations, both geometric and chromatic. Including multiple optical elements into a compound microscope can dramatically improve image quality by providing aberration compensation and enabling telecentricity. Typically, the exact number and design of these components is abstracted to the end-user and can be defined by a relatively low number of descriptive quantities. Magnification and numerical aperture (NA) are the most important of these; the magnification of an objective sets the field of view, while the NA sets a minimum bound on the diffraction-limited resolution. The numerical aperture is defined by the formula $NA=n\sin (\theta)$, where $n$ is the refractive index of the medium, and $\theta$ is the maximum half-angle at which light may pass through the objective relative to the radial (optical) axis. The angular dependence of numerical aperture is completely described by interference effects which arise from the wave-optics model of light propagation. As multiple off-axis sources of the same wavelength converge to a point, the wavefronts of these sources will cause constructive and destructive interference. The minimum distance between two peaks formed by constructive interference is proportional to both the wavelength of the illumination and the angular separation between the two beams (which is set by the maximum NA of the illumination source and imaging optics). Practically, the size of this spot defines the resolution of the optical system. By the Rayleigh criteria, the resolution of an optical system $x_{min}$ is defined as:

\begin{equation}\label{eq:intro:rayleigh}
\Delta x_{min}  = \frac{1.22 \lambda}{(NA_{objective} + NA_{illumination})}
\end{equation}

This quantity defines the minimum separation between two points which can be detected by a system with a circular aperture and is defined by the distance between the center of the point spread function (PSF) and its first null. Note that Eq.~\ref{eq:intro:rayleigh} is dependent on both detection side NA ($NA_{objective}$) and illumination side NA ($NA_{illumination}$).
This equation holds true while $NA_{illumination} \leq NA_{objective}$. High-angle illumination ($NA_{illumination} > NA_{objective}$) provides no direct improvement to the resolution of the measurement, as the signal from these illumination angles is not able to interfere with the background (DC) term. This sets a lower limit on the resolution of a brightfield microscope. When a sample is illuminated from high angles only ($NA_{illumination} > NA_{objective}$), high-resolution features are revealed as diffraction-limited edge contrast but are not individually resolved with more than $2NA_{objective}$ resolution. This is the working principle of darkfield microscopy.

\section{Fourier Optics}
Fourier Optics provides an important bridge between Fourier theory and optical systems by modeling common optical phenomena (such as propagation and lensing) using the Fourier transform. The seminal text on imaging using Fourier theory to analyze imaging systems was published in 1968~\cite{goodman:68} which presents the framework underpinning many common computational techniques such as deconvolution, holography, and free-space propagation. The Fourier optics description is especially useful for an optical system configured as a telecentric ($4f$) system, which is a typical model for an optical microscope:

\begin{figure}[tbh]
\centering
\includegraphics[width=0.4\textwidth]{intro-4f.png}
\caption{\label{fig:4f} Schematic of a 4$f$ optical system}
\end{figure}

\noindent In this system, the lenses in this system operate as forward and inverse Fourier transform operators on the input optical field at (P1) as it propagates through the pupil plane (P2) to the image plane (P3). The electric field at position P2 can be modeled as the Fourier transform of P1, which is often occupied by an aperture stop (circular low-pass filter) to limit the NA of the objective. This stop defines the resolution of the optical system and can be used to reduce aberrations and prevent aliasing of the optical signal at the camera plane. Most modern infinity-corrected microscopes may be approximated to a high degree using these systems, assuming minimal aberrations and misalignment.

\section{Computational Microscopy}

The concept of using computational tools to simulate and invert optical imaging systems was developed soon after the emergence of large-scale computation in the 1960s; the theory of image formation and the propagation of light was developed prior to this time~\cite{maxwell1890scientific, gernheim1969history}. Recently, the field of computational imaging has expanded considerably due to increasing availability of computing power and digital sensing hardware. In modern microscopes, digital cameras allow the detection of the intensity of an optical field using a grid of photodetectors, which digitize the optical field and facilitate computational imaging reconstructions on a host computer. As graphical processing units (GPUs) have become faster and more widely available, computational algorithms have likewise accelerated both in speed and scale.

An early example of computational imaging was the application of a cubic phase plate at the microscope pupil, which provides significantly increased depth of field but produces a highly distorted image. With knowledge of these distortions the original image with extended depth of field can be deconvolved using knowledge of the system's point spread function (PSF)\cite{Dowski:95}. Since these early works, the field of computational microscopy has expanded considerably due to the widespread availability of computing hardware and software tools for simulating optical systems and performing quantitative analysis. Prominent examples include super-resolution methods such as structured illumination~\cite{gustafsson2000surpassing, gustafsson2005nonlinear}, which enhances resolution by projecting a pattern onto the sample, localization microscopy~\cite{betzig2006imaging, Rust:06}, which employs statistical analysis to localize sparse fluorophores using temporal dynamics, and both conventional~\cite{rodenburg2004phase} and Fourier~\cite{Zheng2013} ptychography. Three-dimensional imaging has likewise become a powerful tool for imaging 3D biological quantities, and becomes absolutely necessary for high-$NA$ imaging of thick samples which encounter multiple scattering. Various approaches have been proposed, including deconvolving focal stacks~\cite{agard1984optical}, light-field microscopy~\cite{broxton2013wave, Ng2005}, PSF engineering~\cite{pavani2008three}, and diffraction tomography~\cite{wolf1969three, kim2014diffraction, maleki1992tomographic}. In addition, computational imaging has been widely used for quantitative phase imaging, using interferometry~\cite{Popescu2006,kim2014diffraction, Bhaduri:12}, off-axis holography~\cite{Witte:12}, or commercial add-ons~\cite{phasics,bon2012method}. Another add-on option uses two cameras to capture defocused images which can then be used to solve the Transport of Intensity Equation (TIE)~\cite{allman2005optical}. Alternatively, if chromatic aberrations are large enough, they can enable single-shot color TIE~\cite{wallerColorTIE} without any hardware changes.

In most cases the propagation and refraction of light can be modeled using a small number of linear and non-linear operations and may be accurately simulated using linear algebra software packages such as \texttt{numpy} or \texttt{MATLAB}. With knowledge of this forward model, an inverse problem may be formed to recover the object without any distortions imposed by the imaging system (provided the information is still present in the measurement). In computational imaging these distortions are carefully designed to reveal contrast in ways a conventional system cannot, enabling the recovery of high-dimensional or high-resolution information using computation after an acquisition in performed. The system operator $A\{\cdot\}$ describes and models this entire acquisition process, including a model for the propagation of light, the design of the system, and any calibration or mis-alignment which may be present (Fig.\ref{fig:intro_overview}). To facilitate the recovery of an unknown object from measurements made under this system $y$, an inverse problem formulation is used to invert the system operator $A\{\cdot\}$, often having the common standard form:

\begin{equation}\label{eq:intro_inverse_problem}
\begin{aligned}
& \hat{\vec{x}} = \underset{\vec{\vec{x}}}{\text{argmin}}
& & ||\op{A}\{\vec{x}\} - \vec{y} ||_2^2
\end{aligned}
\end{equation}

\noindent where $\vec{x}$ represents the variable of interest (generally the object), $\op{A}\{\cdot\}$ is the mathematical operation describing the optical system, $\vec{y}$ is the measured intensity, and $\hat{\vec{x}}$ is an estimate of the object $\vec{x}$. The forward operator $\op{A}\{\cdot\}$ is normally formed based on the physics of the optical systems, need not be linear or represented by a matrix.

The goal of a computational imaging system is to invert the forward operator $\op{A}\{\cdot\}$ in a way which minimizes the distance between the object estimate distortions of the forward-inversion process ($||\hat{\vec{x}} - \vec{x}||_2^2$). If $\op{A}\{\cdot\}$ is linear, is can be inverted in a closed-form solution using the Moore-Penrose Pseudo-Inverse~\cite{moore1920reciprocal}, or with an iterative method such as gradient descent. If $\op{A}\{\cdot\}$ is non-linear but smooth, it must be inverted iteratively using analytic expressions for each regularization. If $\op{A}\{\cdot\}$ is non-smooth, it can, in some cases, still be inverted using iterative soft-thresholding methods such as FISTA~\cite{beck2009fast}.

In general, linear problems (characterized by satisfying the relationship $\op{A}\{\vec{a} + \vec{b}\} = \op{A}\{\vec{a}\} + \op{A}\{\vec{b}\} $) are much easier to invert and solve, having lower memory requirements and complexity as well as a direct inverse. Linear convolution operators are particularly common for telecentric imaging systems. When a convolution is well-posed, it may be efficiently inverted using a FFT-based deconvolution algorithm~\cite{cooley1965algorithm}, which has complexity $N \log(N)$ as opposed to $N^2$ for normal operators.

The performance of inversion processes may be improved by adding regularization term to penalize certain undesirable characteristics of the signal, such as noise. The most commonly used regularization method is Tikhonov (or $\ell_2$) regularization~\cite{tikhonov1943stability} which enforces a prior on the total energy of a system. Tikhonov regularization is equivalent to adding an additional $\ell_2$ term to Eq.~\ref{eq:intro_inverse_problem}:

\begin{equation}\label{eq:intro_tikhonov}
\begin{aligned}
& \hat{\vec{x}} = \underset{\vec{x}}{\text{argmin}}
& & ||A\{\vec{x}\}-\vec{y} ||_2^2 + \alpha||\vec{x}||_2^2
\end{aligned}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/fig_intro_comp_imaging.pdf}
    \label{fig:intro_overview}
    \caption{Overview of Computational Imaging. The forward model $\op{A}$ is a function of the physical properties of light, the optical system design, and (mis)calibration of the system. An image of a Nikon TE300 microscope used in this work is provided for context.}
\end{figure}


\noindent where $\alpha$ is a tuning parameter which represents the weight of the Tikhonov prior (normally set to $\frac{1}{SNR}$). If $\op{A}\{\cdot\}$ is linear and can be represented as a matrix, Eq.~\ref{eq:intro_tikhonov} can be directly inverted using a closed-form expression:

\begin{equation}
    \hat{\vec{x}} = ((\mat{A}^H \mat{A})^{-1} + \alpha \mat{I})\mat{A}^H \vec{y}
\end{equation}

\noindent where $\mat{A}$ is the matrix form of $\mat{A}\{\cdot\}$ and $\mat{I}$ is the identity matrix with the same dimensions as $\mat{A}^H \mat{A}$. This closed-form solution makes Tikhonov regularization popular for many inverse problems, although the total energy prior may not be accurate in all cases.

A second common class of priors enforce sparsity of the object in some domain. Mathematically the $\ell_0$ "norm" returns the number of non-zero elements of the input. This norm is non-convex, however, requiring a large combinatorial search which is intractable for most problems~\cite{candes2008enhancing}. As a proxy, the $\ell_1$ norm is conventionally employed as a convex, though non-smooth alternative~\cite{taylor1979deconvolution}. When coupled with a generalized sparsifying operator $\op{W}\{\cdot\}$ and a differential forward model $\op{A}\{\cdot\}$, this problem is convex, and can be written as:

\begin{equation}\label{eq:intro_sparse}
\begin{aligned}
& \hat{\vec{x}} = \underset{\vec{x}}{\text{argmin}}
& & ||\op{A}\{x\}-\vec{y} ||_2^2 + \alpha||\op{W}\{\vec{x}\}||_1.
\end{aligned}
\end{equation}

Because the regularization term is non-smooth, iterative solvers must be used to recover the optimal $\hat{\vec{x}}$. When $\op{W}$ is a unitary function $\mat{w}$ (or the identity matrix), a solver implementing proximal gradient descent using soft-thresholding may be used to minimize this objective function, such as FISTA~\cite{beck2009fast}, ADMM~\cite{boyd2011distributed}, or TwIST~\cite{bioucas2007new}. In general, $\mat{W}$ can be any unitary transform, including the Fourier transform  or Wavelet Transform, or a learned unitary operator which is optimized using a machine-learning framework~\cite{ravishankar2013learning}. In all of these cases, the optimal $\hat{\vec{x}}$ may be recovered by performing many iterations of proximal gradient descent. In the case where $\mat{W}$ is not unitary, the above relationship does not hold, and other proximal methods must be used. One prominent example is total-variation regularization (TV), which enforces sparsity of the image gradients~\cite{rudin1992nonlinear}. TV regularization can be implemented using ADMM~\cite{wahlberg2012admm}, FISTA~\cite{beck2009fast}, or using soft-thresholding on wavelet coefficients~\cite{kamilov2012wavelet}.

\section{Noise in Computational Microscopy Systems}\label{sec:intro_noise}
All measurements contain noise from various sources, including photon quantization or camera electronics. In general, these noise sources can be additive or multiplicative, and may take on a variety of statistical profiles including Gaussian and Poisson distributions. Analyzing the propagation of noise through computational imaging systems is extremely important for practical implementation of the algorithms presented in this work, as they help motivate when computational imaging makes sense. Analyses previously introduced in computational photography~\cite{cossairt2013does} have been adopted for microscopy applications here. In this section, we provide a derivation which applies to all linear systems presented in this dissertation, particularly Chapter~\ref{ch:phase} and Chapter~\ref{ch:highthroughput}. Here, we generally assume the presence of an additive, Gaussian noise term $\vec{\eta}$ with zero-mean, and variance $\sigma_{\vec{\eta}}$, which is added to each measurement made under a general forward operator $\op{A}\{\cdot\}$:

\begin{equation}\label{eq:intro_forward_model}
    \vec{y} = \op{A}\{\vec{x}\} + \vec{\eta}
\end{equation}

This approximation is normally valid for measurements made with more than 10 photon counts, which includes every case presented in this dissertation (including fluorescence imaging). The effect of this noise on image quality is generally represented as the signal-to-noise ratio (SNR). Here, we use the common imaging SNR definition:

\begin{equation}
    \label{eq:intro_snr}
    SNR = \frac{\bar{\vec{y}}}{\sigma_{\vec{\eta}}}
\end{equation}

\noindent where $\sigma_{\vec{\eta}}$ is the standard deviation of the noise term and $\bar{y}$ is the mean signal (DC term) of the measurement $y$. When inverting the forward operator $\op{A}\{\}$ to recover $\vec{x}$, the presence of $\vec{\eta}$ will lead to error in the measurements compared to the ground truth. Take, for example, if $\op{A}$ can be represented as a matrix $\mat{A}$, the Moore-Penrose pseudoinverse of the object is given by:

\begin{equation}\label{eq:intro_noise_inverse}
        \hat{\vec{\vec{x}}} = \vec{x} + (\mat{A}^H \mat{A})^{-1} \mat{A}^H \vec{\eta}.
\end{equation}

\noindent Clearly, the recovered object $\hat{\vec{x}}$ will be corrupted by an additive term, which is essentially the inversion process applied to the noise term $\hat{\vec{\eta}}$. Based on Eq.~\ref{eq:intro_noise_inverse}, the root-mean-squared error (RMSE) between $\hat{\vec{x}}$ and the true $\vec{x}$ is $(\mat{A}^H \mat{A})^{-1} \mat{A}^H \vec{\eta}$. Taking the covariance of this term, we can find an expression for the covariance of the error term in the reconstruction:

\begin{equation}\label{eq:intro_noise_covariance}
    \mat{\Sigma}_{\mat{A}^{\dagger}\vec{\eta}} = \sigma_{\vec{\eta}}^2 (\mat{A}^H \mat{A}) ^{-H}
\end{equation}

The main result of Eq.~\ref{eq:intro_noise_covariance} is that the inversion process re-weights the spectrum of the original Gaussian white noise $\vec{\eta}$. When $\mat{A}$ has an $\ell_2$ operator norm of 1, the minimum singular value of $\mat{A}$ defines the maximum noise amplification, while the sum of the inverse singular values defines the total RMSE:

\begin{equation}\label{eq:intro_noise_amplification}
    \sigma_{\mat{A}^{\dagger}\vec{\eta}} = \sigma_{\vec{\eta}} \sqrt{\mat{\Sigma}_{i=0}^N{\frac{1}{\sigma_i^2\{\mat{A}\}}}},
\end{equation}

\noindent where $N$ is the length of $\vec{x}$ and $\sigma_i^2 \{\mat{A}\}$ represents the $i^{th}$ singular value of $\mat{A}$. This definition is consistent with~\cite{agrawal2009optimal}. This relationship between the singular values of $A$ and the amplification of $\vec{\eta}$ enables a closed-form relationship to the reconstruction signal-to-noise ratio ($SNR_{recon}$) of a measurement $\vec{y}$, defined as:

\begin{equation}\label{eq:intro_snr_recon}
SNR_{recon} = \frac{\bar{\vec{y}}}{\sigma_{\vec{\eta}} \sqrt{\mat{\Sigma}_{i=0}^N{\frac{1}{\sigma_i^2\{\mat{A}\}}}}},
\end{equation}

\noindent From this relationship, it becomes clear that the deconvolution process will reduce the $SNR$ by a factor:

\begin{equation}\label{eq:intro_dnf}
    f = \sqrt{\mat{\Sigma}_{i=0}^N{\frac{1}{\sigma_i^2\{\mat{A}\}}}},
\end{equation}

\noindent where $f$ is the deconvolution noise factor (DNF).

The main result here is that the deconvolution process will always amplify measurement noise, and that for linear systems this amplification can be computed algebraically so long as the singular values of the forward operator are known. This result is particularly useful for convolutional forward operators, where the singular values of $A$ can be computed quickly and efficiently from the Fourier coefficients of the convolution kernel $h$ due to the circulant structure of $A$. This analysis of RMSE amplification is equivalent to A-optimal design~\cite{chernoff1953locally}, which is used here due to strict compatibility with the definition of imaging SNR (Eq.~\ref{eq:intro_snr}).

\begin{figure}[tbh]
\centering
\includegraphics[width=0.96\textwidth]{figures/fig_intro_dnf.pdf}
\caption{\label{fig:intro:dnf} Simulation of a convolutional forward model and verification of DNF calculations. Top row shows forward modal with additive Gaussian noise, while the middle row shows the deconvolution of the same measurement, separated into object deconvolution and noise amplification terms. Bottom row shows the measurement and reconstruction SNR across 1000 random generations of the Gaussian white noise term, overlaid with the multiplication of the reconstruction SNR multiplied by the DNF as a verification. The operators $*$ and $*^{-1}$ represent 2D convolution and deconvolution, respectively.}
\end{figure}

As a demonstration of this relationship, we simulated a convolutional forward model with additive Gaussian noise, and perform deconvolution of the noisy measurement. These results show that the DNF provides a very close scalar relationship between measurement SNR and deconvolved SNR, which is illustrated by comparing the estimated measurement SNR (deconvolved SNR multiplied by the DNF) to the original measurement SNR. Small discrepancies between the expected and predicted SNR calculations are likely due to sampling error, since we evaluate the noise standard deviation across a 20-pixel square in the top left corner of each image, rather than the full image (to avoid including the standard deviation of the object in our SNR calculation). These relationships are used in both Chapter~\ref{ch:phase} and Chapter~\ref{ch:highthroughput} for analyzing the noise propagation of linear forward models.

\section{Coded Illumination for Optical Microscopy}

Since the 17$^{th}$ century, microscopes have employed some sort of light source to illuminate a specimen, from candles to semiconductor light sources. Illumination affects image contrast in variety of ways - for purely absorptive samples, brightfield microscopy images the light that is attenuated by the sample by illuminating within the range of angles defined by the numerical aperture ($NA$) of the objective. Conversely, darkfield microscopy uses illumination from angles outside of the illumination $NA$, imaging only light that is scattered or "bent" by a refractive medium such as water. Phase contrast microscopy~\cite{zernike1942phase} uses annular illumination to reveal qualitative phase contrast, while differential interference contrast (DIC)~\cite{smithDIC} uses coherent, polarized illumination to reveal the gradient of an object's phase in a single direction. Recently, programmable light sources such as LED arrays~\cite{Zheng2011, albeanu2008led} have enabled a variety of qualitative contrast modalities at high speed~\cite{Zheng2011, zijiMulti}, as well as the capability to perform quantitative reconstructions of the complete complex field of the sample~\cite{Tian3dDpc, Zheng2013, tian2015quantitative}, enabling the measurement of the dry mass of many aqueous biological samples~\cite{popescu2008imaging, popescu2008optical}.

In the following chapters, I will describe several novel applications and of coded illumination in optical microscopy, fabrication methods for coded illumination devices, and self-calibration techniques for the aforementioned methods, Fig.~\ref{fig:intro_system_dome} shows the system which was used in most experiments presented in this dissertation.

Chapter~\ref{ch:phase} describes methods for qualitative and quantitative phase recovery, including a single-shot quantitative phase imaging method which uses partially coherent color-multiplexed illumination to recover the complete optical field of an object from a single measurement. Chapter~\ref{ch:highthroughput} describes a novel method for recovering a large field-of view with high SNR by using a coded illumination sequence to introduce motion blur during each capture, which is then computationally removed using a motion deblurring algorithm to recover the static object. We demonstrate the performance of this technique for both brightfield imaging and fluorescence imaging and provide an analysis of optimal acquisition strategy in terms of common system parameters such as camera noise level and illumination power. In Chapter~\ref{ch:fabrication}, we describe the fabrication of several prototypes used for coded illumination, including a programmable domed LED array, LED sources for high-throughput imaging, and Computational CellScope, a prototype device which uses a programmable domed LED illumination to perform quantitative phase imaging, digital refocusing, and multi-contrast imaging in a portable (smartphone-based) form factor. Chapter~\ref{ch:selfcal} describes self-calibration techniques for quantitative phase imaging, including LED position recovery for LED domes and aberration recovery using a linearized model. These methods are essential for practical implementation of many quantitative phase imaging techniques. Chapter~\ref{ch:conclusion} concludes this dissertation on quantitative microscopy using coded illumination and provides future extensions of the work presented in the previous chapters.

\begin{figure}[bh]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/fig_intro_dome_system.pdf}
    \label{fig:intro_system_dome}
    \caption{Nikon TE300-based system used for most experiments presented in this dissertation. This system uses a digital camera, programmable LED illumination source (Quasi-dome), mechanical motion stage, and acquisition PC for controlling the acquisition and processing the data.}
\end{figure}
